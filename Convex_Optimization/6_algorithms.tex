% !TEX root = main.tex

\section{优化算法}
\subsection{简介}
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{A\vx}{=\vb}
\end{mini*}
罚函数法
\[\min f_0(x)+\frac{\lambda}{2}\norm{A\vx-\vb}_2^2\]
\[\tilde{x}=\arg\min_x F\]
\[\nabla f_0(\tilde{\lambda})+\lambda A^\T(A\tilde{\vx}-\vb)=0\]
\[\begin{aligned}
    L(x,v)&=f_0(x)+v^\T(A\vx-\vb)\\
    \implies g(v)&=\inf_x f_0(x)+v^\T(A\vx-\vb)\\
    v&=\lambda(A\tilde{\vx}-\vb)\\
    \implies & g(\lambda(A\tilde{x}-\vb)=\inf_x f_0(x)+\lambda(A\tilde{x}-b)^\T(A\vx-\vb)
\end{aligned}\]
\[\nabla f_0(x)+\lambda A^\T(A\tilde{x}-\vb)=0\]

\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{A\vx}{\geq\vb}
\end{mini*}
log-barrier
\[\min f_0(x)+\sum_{i=1}^m u_i\log(a_i^\T \vx-b_i)\]


$\min f_0(x)$可微，凸，无约束
\begin{enumerate}
    \item 所有算法都是迭代的
    \[\iter{x}{k+1}=\iter{x}{k}+\iter{\alpha}{k}\iter{d}{k}\]
    $\alpha\geq 0$为步长，$d$为方向，所有算法本质上都是选择方向与步长的问题
    \item 如何选择步长$\iter{\alpha}{k}$
    \[\begin{cases}
    \text{确定步长} & \begin{cases}\text{固定步长}\\\text{变化步长（递减步长）}\end{cases}\\
    \text{搜索步长}
    \end{cases}\]
    最优步长：线搜索问题
    \[\iter{\alpha}{k}=\arg\min_{\alpha\geq 0} f_0(\iter{x}{k}+\alpha\iter{d}{k})\]
    \item 关键问题是选方向
\end{enumerate}
黄金分割法(0.618法)/优选法求解线搜索问题：这样做的采样复杂度很低，之前算过的点很容易被再用！

不精确线搜索(Armijo Rule)：一阶泰勒展开
% \begin{algorithm}[H]
%     \begin{algorithmic}[1]
%         \State{$\iter{\alpha}{k}=\alpha_\max$}
%         \If{$f_0(\iter{x}{k}+\iter{\alpha}{k}\iter{d}{k})>f_0(\iter{x}{k}+\mu\iter{\alpha}{k}\lrang{\nabla f_0(\iter{x}{k},\iter{d}{k})})$}
%         \State $\iter{\alpha}{k}\gets\iter{\alpha}{k}\beta,\beta\in(0,1)$
%         \Else
%         \State Stop
%         \EndIf
%     \end{algorithmic}
% \end{algorithm}

实际上没有必要求最优步长，在该方向上的差异并没有太大

\subsection{梯度下降法}
$\iter{d}{k}=-\nabla f_0(\iter{x}{k})$
\begin{itemize}
    \item 能否收敛
    \item 收敛到哪里
    \item 收敛速度
\end{itemize}

假设
\begin{itemize}
    \item[0.] 基本假设：$f$为可微的凸函数，
    \[x^\star=\arg\min_x f_0(x)\]
    存在且有限，$f_0(x^\star)$有限
    \item[1.] Lipschitz连续梯度
    \[\exists L\geq 0,\norm{\nabla f_0(x)-\nabla f_0(y)}\leq L\norm{x-y},\forall x,y\]
    等价定义：
    \begin{itemize}
        \item[a.]若$f_0(x)$二阶可微
    \[\nabla^2f_0(x)\preceq LI,\forall x\]
        \item[b.] 下界
    \[\lrang{\nabla f_0(x)-\nabla f_0(y),x-y}\geq\frac{1}{L}\norm{\nabla f_0(x)-\nabla f_0(y)}^2\]
        \item[c.] 上界
    \[\lrang{\nabla f_0(x)-\nabla f_0(y),x-y}\leq L\norm{x-y}\]
        \item[d.] 当函数为凸时
    \[0\leq f_0(y)-f_0(x)-\lrang{\nabla,y-x}\leq\frac{L}{2}\norm{x-y}_2^2\]
    \end{itemize}
    \item[2.] 强凸性(strong convexity)
    \[\exists\mu>0:\;f_0(y)\geq f_0(x)+\lrang{\nabla f_0(x),y-x}+\frac{\mu}{2}\norm{x-y}_2^2,\forall x,y\]
    二阶可微情况下的等价定义
    \[\nabla^2 f(x)\succeq \mu I\]
\end{itemize}
\begin{example}
    \[\begin{aligned}
        f_0(x) &= \vone^\T x\qquad & L &= 0 & \text{\xmark}\\
        f_0(x) &= \frac{1}{2}\norm{x}_2^2 & L &=1 & \mu=1\\
        f_0(x) &= \frac{1}{4}\norm{x}_2^4 & \text{\xmark} & &\text{\xmark}
    \end{aligned}\]
\end{example}

区别于严格凸(strictly convex)，强凸一定是严格凸
\begin{theorem}
    严格凸函数只有一个最小值点
\end{theorem}
\begin{analysis}
    反证法，假设$x,y$均为最小值点，且$x\ne y$
    \[f_0(y)>f_0(x)+\lrang{\nabla f_0(x),x-y}=f_0(x)\]
\end{analysis}

\begin{theorem}
    若$f_0(x)$有Lipschitz连续梯度，常数$L$，若$\alpha\in(0,\frac{2}{L})$，则有
    \[\begin{aligned}
        f_0(\iter{x}{k})-f_0(x^\star)&\leq \frac{2(f_0(x^0)-f_0(x^\star))\norm{x^0-x^\star}^2}
        {2\norm{x^0-x^\star}^2+k\alpha(2-L\alpha)(f_0(x^0)-f_0(x^\star))},\forall x^\star
    \end{aligned}\]
    即以$O(\frac{1}{k})$速度收敛
\end{theorem}
\begin{analysis}
    $1\degree$ 点的单调性：与任意$x^\star$的距离在缩小
    \[\norm{\iter{x}{k+1}-x^\star}^2\leq\norm{\iter{x}{k}-x^\star}^2,\forall x^\star\]
    \[\begin{aligned}
        LHS &= \norm{\iter{x}{k}-x^\star-\alpha\nabla f_0(x^k)}^2\\
        &=\norm{\iter{x}{k}-x^\star}^2-2\alpha\lrang{x^k-x^\star,\nabla f_0(x^k)}+\alpha^2\norm{\nabla f_0(x^k)}\\
        &\leq \norm{\iter{x}{k}-x^\star}^2+\alpha(\alpha-\frac{2}{L})\norm{\nabla f_0(\iter{x}{k})}^2\qquad\text{注意到$\nabla f_0(x^\star)=0$，利用Lipschitz连续梯度}\\
        &\leq \norm{\iter{x}{k}-x^\star}^2
    \end{aligned}\]
    $2\degree$ 函数值的单调性：$f_0(\iter{x}{k+1})\leq f_0(\iter{x}{k})$（注意下降可能非常缓慢，并不一定收敛）
    \[\begin{aligned}
        f_0(\iter{x}{k+1})&\leq f_0(\iter{x}{k})+\lrang{\nabla f_0(\iter{x}{k}),\iter{x}{k+1}-\iter{x}{k}}+\frac{L}{2}\norm{\iter{x}{k+1}-\iter{x}{k}}^2\\
        &= f_0(\iter{x}{k})-\alpha(1-\frac{L\alpha}{2})\norm{\nabla f_0(\iter{x}{k})}^2\\
        &\leq f_0(\iter{x}{k})
    \end{aligned}\]
    $3\degree$ 函数值的充分下降（即证明收敛性）
    \[f_0(\iter{x}{k+1})-f_0(x^\star)\leq f_0(\iter{x}{k})-f_0(x^\star)-\omega\norm{\nabla f_0(\iter{x}{k})}^2\]
    \[\begin{aligned}
        f_0(\iter{x}{k})-f_0(x^\star)&\leq \lrang{f_0(\iter{x}{k},\iter{x}{k}-x^\star)}\\
        &=\lrang{\nabla f_0(\iter{x}{k})-\nabla f_0(x^\star),\iter{x}{k}-x^\star}\\
        &\leq\norm{\nabla f_0(\iter{x}{k})-\nabla f_0(x^\star)}\norm{\iter{x}{k}-x^\star}\\
        &\leq \norm{\nabla f_0(\iter{x}{k})}\norm{\iter{x}{k}-x^\star}
    \end{aligned}\]
    \[\iter{\Delta}{k+1}\leq \iter{\Delta}{k}-\frac{\omega}{\norm{x^0-x^\star}^2}(\iter{\Delta}{k})^2\]
    \[\frac{1}{\iter{\Delta}{k+1}}\leq\frac{1}{\iter{\Delta}{k+1}}-\frac{\omega}{\norm{x^0-x^\star}^2}\frac{\iter{\Delta}{k}}{\iter{\Delta}{k+1}}\]
    错位相消可得结论$O(\frac{1}{k})$收敛速度
\end{analysis}

\begin{theorem}
    若$f_0$有Lipschitz连续梯度，常数$L$，强凸函数$n$，步长$\alpha\in(0,\frac{2}{\mu+L}]$，则
    \[\norm{\iter{x}{k}-x^\star}^2\leq\lrp{1-\frac{2\alpha\mu L}{\mu+L}}^k\norm{\iter{x}{0}-x^\star}^2\]
\end{theorem}
\begin{analysis}
\[\begin{aligned}
    \norm{\iter{x}{k}-x^\star}^2 &= \norm{\iter{x}{k}-\alpha\nabla f_0(\iter{x}{k})-x^\star}^2\\
    &= \norm{\iter{x}{k}-x^\star}^2-2\alpha\lrang{\iter{x}{k}-x^\star,\nabla f_0(\iter{x}{k})}+\alpha^2\norm{\nabla f_0(\iter{x}{k})}^2\\
    &\leq \norm{\iter{x}{k}-x^\star}^2-\frac{2\alpha}{\mu+L}\norm{\nabla f_0(x)}^2+\alpha^2\norm{\nabla f_0(\iter{x}{k})}^2 \qquad\text{内积不等式}\\
    &\leq RHS
\end{aligned}\]
\[1-\frac{4\mu L}{(\mu+L)^2}=\frac{(L-\mu)^2}{(L+\mu)^2}=\frac{\lrp{\frac{L}{\mu}-1}^2}{\lrp{\frac{L}{\mu}+1}^2}\]
$L$为Hessian矩阵的最大特征值，$\mu$为Hessian矩阵的最小特征值，则$\frac{L}{\mu}$为该矩阵的条件数
\end{analysis}

不同收敛速度
\begin{itemize}
    \item 次线性收敛
    \item 线性收敛
    \item 超线性收敛
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{fig/linear.png}
\end{figure}

\begin{example}
\[f_0(x)=\frac{1}{2}\norm{Ax-b}_2^2\]
\end{example}
\begin{analysis}
    \[\iter{x}{0}\to\iter{x}{1}\]
    \[\iter{x}{1}=\iter{x}{0}-\alpha(\iter{x}{0}-b)=b\]
\end{analysis}

条件数糟糕的病态矩阵收敛速度是非常糟糕的，会出现zig-zag的情况
\[A=\bmat{1 & 0\\ 0 & 10^{-4}}\]
可以通过预处理(precondition)来解决条件数糟糕的问题

$f_0$，Lipschitz连续梯度($L$)，强凸($\mu$)，函数值收敛性
\[\begin{aligned}
    \tilde{f}_0(\iter{\alpha}{k})&=f_0(\iter{x}{k+1})=f_0(\iter{x}{k}-\iter{\alpha}{k}\nabla f_0(\iter{x}{k}))\\
    &\leq f_0(\iter{x}{k})+\lrang{\nabla f_0(\iter{x}{k}),-\iter{\alpha}{k}\nabla f_0(\iter{x}{k})}+\frac{L}{2}\norm{-\iter{\alpha}{k}\nabla f_0(\iter{x}{k})}^2\\
    &=f_0(\iter{x}{k})+\frac{L(\iter{\alpha}{k})^2-2\iter{\alpha}{k}}{2}\norm{\nabla f_0(\iter{x}{k})}^2
\end{aligned}\]

$\iter{\alpha}{k}=\iter{\alpha}{k}_{exact}$精确线搜索
\[\begin{aligned}
    &\tilde{f}_0(\frac{1}{L})=f_0(\iter{x}{k})-\frac{1}{2L}\norm{\nabla f_0(\iter{x}{k})}^2\\
    &\tilde{f}_0(\iter{\alpha}{k}_{exact})\leq\tilde{f}_0(\frac{1}{L})\\
    \implies&\tilde{f}_0(\iter{\alpha}{k}_{exact})-f_0(\iter{x}{k})-f_0(x^\star)-\frac{1}{2L}\norm{\nabla f_0(\iter{x}{k})}^2\\
    &\leq (1-\frac{\mu}{L})(f_0(\iter{x}{k})+f_0(x^\star))
\end{aligned}\]
\[\begin{aligned}
    f_0(x^\star)&\geq f_0(\iter{x}{k})+\lrang{\nabla f_0(\iter{x}{k}),x^\star-\iter{x}{k}}+\frac{\mu}{2}\norm{\iter{x}{k}-x^\star}^2\\
    &\geq f_0(\iter{x}{k})-\frac{\mu}{2}\norm{\iter{x}{k}-x^\star}^2-\frac{1}{2\mu}\norm{\nabla f_0(\iter{x}{k})}^2+frac{\mu}{2}\norm{\iter{x}{k}-x^\star}^2\qquad ab\geq-\frac{\mu}{2}a^2-\frac{1}{2\mu}b^2\\
    &= f_0(\iter{x}{k})-\frac{1}{2\mu}\norm{\nabla f_0(\iter{x}{k})}^2
\end{aligned}\]
\[f_0(\iter{x}{k}-f_0(x^\star)\leq\frac{1}{2\mu}\norm{\nabla f_0(\iter{x}{k})}^2\]

Armijo Rule
\[\tilde{f}_0(\iter{\alpha}{k})=f_0(\iter{x}{k+1})\leq f_0(\iter{x}{k})+\frac{L(\iter{\alpha}{k})^2-2\iter{\alpha}{k}}{2}\norm{\nabla f_0(\iter{x}{k})}^2\]
\[\tilde{f}_0(\iter{\alpha}{k})=f_0(\iter{x}{k+1})\leq f_0(\iter{x}{k})-\nabla\iter{\alpha}{k}\norm{\nabla f_0(\iter{x}{k})}^2\]
首先说明，若$0\leq\iter{\alpha}{k}\leq\frac{1}{L}$时，则
\[\tilde{f}_0(\iter{\alpha}{k})\leq f_0(\iter{x}{k})-\gamma\iter{\alpha}{k}\norm{\nabla f_0(\iter{x}{k})}^2\]
当$\iter{\alpha}{k}\in[0,\frac{1}{2}]$时，
\[-\iter{\alpha}{k}+\frac{L}{2}(\iter{\alpha}{k})^2\leq -\frac{\iter{\alpha}{k}}{2}\iff \frac{L}{2}(\iter{\alpha}{k})^2\leq\frac{\iter{\alpha}{k}}{2}\iff L\cdot\iter{\alpha}{k}\leq 1\]
\[\begin{aligned}
    f_0(\iter{x}{k+1})&\leq f_0(\iter{x}{k})+\frac{L(\iter{\alpha}{k})^2-2\iter{\alpha}{k}}{2}\norm{\nabla f_0(\iter{x}{k})}^2\\
    &\leq f_0(\iter{x}{k})-\frac{\iter{\alpha}{k}}{2}\norm{\nabla f_0(\iter{x}{k})}^2\\
    &\leq f_0(\iter{x}{k})-\gamma\iter{\alpha}{k}\norm{\nabla f_0(\iter{x}{k})}^2
\end{aligned}\]
\[\begin{aligned}
    &f_0(\iter{x}{k+1})\leq f_0(\iter{x}{k})-\min\{\gamma\alpha_{\max},\frac{\gamma\beta}{L}\}\norm{\nabla f_0(\iter{x}{k})}^2\\
    &\implies f_0(\iter{x}{k+1})-f_0(x^\star)\leq\lrp{1-\min\{2\mu\gamma\alpha_{\max},\frac{2\mu\gamma\beta}{L}\}}(f_0(\iter{x}{k}-f_0(x^\star)))
\end{aligned}\]

梯度下降法的解释1
\[\iter{x}{k+1}=\iter{x}{k}-\iter{\alpha}{k}\nabla f_0(\iter{x}{k})\]
将$f_0$在$\iter{x}{k}$处进行一阶Taylor展开
\[f_0(x)\approx f_0(\iter{x}{k})+\lrang{\nabla f_0(\iter{x}{k}),x-\iter{x}{k}}+\frac{1}{2\iter{\alpha}{k}}\norm{x-\iter{x}{k}}^2\]
求梯度
\[\begin{aligned}
    \nabla f_0(\iter{x}{k})+\frac{1}{\iter{\alpha}{k}}(x-\iter{x}{k})&=0\\
    \iter{\alpha}{k}\nabla f_0(\iter{x}{k})+x-\iter{x}{k}&=0\\
    x=\iter{x}{k}&-\iter{\alpha}{k}\nabla f_0(\iter{x}{k})
\end{aligned}\]

解释2
\[f_0(\iter{x}{k}+v)\approx f_0(\iter{x}{k})+\lrang{\nabla f_0(\iter{x}{k}),v}\]
\[\iter{d}{k}=\arg\min_v\{\lrang{\nabla f_0(\iter{x}{k}),v}\mid \norm{v}=1\}\]
若采用2-范数，可得标准化的负梯度方向(normalized negative gradient)
\[\iter{d}{k}=\frac{-\nabla f_0(\iter{x}{k})}{\norm{\nabla f_0(\iter{x}{k}}_2}\]
通过改变不同的范数，有不同的特性

坐标下降法(coordinate descent/alternating direction)交替极小化
\[\iter{d}{k}=\ve_{\mod{}{}}(k,n)\]
注意，这里$x\in\rn$，$n \mod n = n$
\[\iter{\alpha}{k}=\arg\min f_0(\iter{x}{k}+\alpha\iter{d}{k}),\alpha_{\max}\geq \alpha\geq \alpha_{\min}\]

\subsection{非光滑优化问题}
\subsubsection{次梯度法}
\[\min f_0(x),\qquad f_0\text{连续，凸，不可微}\]
梯度\textbf{下降}法$\to$次梯度(subgradient)法\\
$g_0(x)\in\partial f_0(x)$（注意凹函数则对应的是supgradient）
\[f_0(y)\geq f_0(x)+\lrang{g_0(x),y-x},\forall y\]
$f(x)=|x|$在零点处次梯度为$[-1,1]$
\[\iter{x}{k+1}=\iter{x}{k}-\iter{\alpha}{k}g_0(\iter{x}{k})\]
只要有$0\in\partial f_0(x_0)$就有最优解$x=x_0$

如果激活函数为非光滑的（如ReLU)，那么出来的函数也是非光滑的，就要用次梯度

关键在于选择步长
\begin{itemize}
    \item 固定步长$\iter{\alpha}{k}=\alpha$
    \item 不可加但平方可加，如$\frac{1}{k}$
    \[\sum_{k=0}^\infty \iter{\alpha}{k}=\infty\qquad\sum_{k=0}^\infty(\iter{\alpha}{k})^2<\infty\]
    \item 不可加递减，如$\frac{1}{\sqrt{k}}$
    \[\sum_{k=0}^\infty\iter{\alpha}{k}=0\qquad\lim_{k\to\infty}\iter{\alpha}{k}\to 0\]
\end{itemize}

\[\inf_{i=0,\ldots,k}(f_0(\iter{x}{i})-f_0(x^\star))\]
\[\iter{\bar{x}}{k}:=\frac{\sum_{i=0}^k\iter{\alpha}{i}\iter{x}{i}}{\sum_{i=0}^k\iter{\alpha}{i}}\]
\[f_0(\iter{\bar{x}}{k})-f_0(x^\star)\]

假设函数Lipschitz连续
\[\exists G>0,\forall x,y:\;\norm{f_0(x)-f_0(y)}\leq G\norm{x-y}\]
$\forall x^\star$最优
\[\begin{aligned}
    &\qquad\norm{\iter{x}{k+1}-x^\star}^2=\norm{\iter{x}{k}-\iter{\alpha}{k}g_0(\iter{x}{k})-x^\star}^2\\
    &=\norm{\iter{x}{k}-x^\star}^2-2\lrang{\iter{\alpha}{k}g_0(\iter{x}{k}),\iter{x}{k}-x^\star}+(\iter{\alpha}{k})^2\norm{g_0(\iter{x}{k})}^2\\
    &\leq \norm{\iter{x}{k}-x^\star}^2-2(\iter{\alpha}{k})^2(f_0(\iter{x}{k})-f_0(x^\star))+(\iter{\alpha}{k})^2 G^2\\
    &\implies \norm{\iter{x}{k+1}-x^\star}^2\leq \norm{\iter{x}{0}-x^\star}^2-2\sum_{i=0}^k\iter{\alpha}{i}(f_0(\iter{x}{i})-f_0(x^\star))+\sum_{i=0}^k(\iter{\alpha}{i})^2 G^2\\
    &\implies 2\sum_{i=0}^k\iter{\alpha}{i}(f_0(\iter{x}{i})-f_0(x^\star))\leq \norm{\iter{x}{0}-x^\star}^2+\sum_{i=0}^k(\iter{\alpha}{i})^2G^2
\end{aligned}\]
\[\sum_{i=0}^k\iter{\alpha}{i}(f_0(\iter{x}{i})-f_0(x^\star))\geq\lrp{\sum_{i=0}^k\iter{\alpha}{i}}\inf_{i=0,\ldots,k}(f_0(\iter{x}{i})-f_0(x^\star))\]
\[\implies\inf_{i=0,\ldots,k}(f_0(\iter{x}{i})-f_0(x^\star))\leq\frac{\norm{\iter{x}{0}-x^\star}^2+G^2\sum_{i=0}^k(\iter{\alpha}{i})^2}{2\sum_{i=0}^k\iter{\alpha}{i}}\]
这是一个紧的界
\begin{itemize}
    \item 固定步长得到上界$\frac{G^2\alpha}{2}$，以$f(x)=|x|$为例
    \item 不可加平方可加一定收敛，若步长为$\frac{1}{k}$，收敛速度为$\frac{1}{\log k}$（幂级数积分取上下界$\sum_{i=0}^k\frac{1}{i}=O(\log k)$）
    \item 不可加平方不可加同样收敛，若步长为$\frac{1}{\sqrt{k}}$，收敛速度为$O(\frac{\log k}{\sqrt{k}})$，可以证明在该假设下该收敛速度最优
\end{itemize}

\[\lim_{k\to\infty}\inf_{i=0,\ldots,k}\lrp{f_0(\iter{x}{i})-f_0(x^\star)}=0\]
\[\forall \eps>0,\exists N_1\in\zz, \iter{\alpha}{i}\leq\frac{\eps}{G^2},\forall i>N_1\]
\[\exists N_2\in\zz:\sum_{i=0}^k\iter{\alpha}{i}\geq\frac{1}{\eps}\lrp{\norm{\iter{x}{0}-x^\star}^2+G^2\sum_{i=0}^{N_1}(\iter{\alpha}{i})^2},\forall k>N_2\]
另$N=\max\{N_1,N_2\},\forall k>N$
\[\frac{\norm{\iter{x}{0}-x^\star}^2+G^2\sum_{i=0}^{N_1}(\iter{\alpha}{i})^2}{2\sum_{i=0}^k\iter{\alpha}{i}}+\frac{\norm{\iter{x}{0}-x^\star}^2+G^2\sum_{i=N_1+1}^{k}(\iter{\alpha}{i})^2}{2\sum_{i=0}^{N_1}\iter{\alpha}{i}+2\sum_{i=N_1+1}^{k}\iter{\alpha}{i}}\leq\frac{\eps}{2}+\frac{\eps}{2}=\eps\]
\[\text{第二项}\leq\frac{G^2\sum_{i=N_1+1}^k\iter{\alpha}{i}\frac{\eps}{G^2}}{2\sum_{i=0}^{N_1}\iter{\alpha}{i}+2\sum_{i=N_1+1}^{k}\iter{\alpha}{i}}\leq\frac{\eps}{2}\]
实际上这个假设一般情况下不成立，但是我们只需保证在优化路径上成立即可，也有设置$x$有界的

\subsubsection{邻近点梯度法(proximal gradient method)}
有\textbf{结构}，不可微

\[\min f_0(x)=s(x)+r(x)\]
\begin{itemize}
    \item s: smooth，可微，易求导
    \item r: regularization，不可微，易求邻近点投影
\end{itemize}

\[r(x),\hat{x}\cdot\alpha>0\]
邻近点投影(proximal mapping)
\[\min_x r(x)+\frac{1}{2\alpha}\norm{x-\hat{x}}^2\]
\begin{example}[LASSO]
    \[f_0(x)=\frac{1}{2}\norm{y-Ax}^2+\lambda\norm{x}_1\]
\end{example}
\begin{analysis}
    本来想优化0-范数，但因为不好做，故变为优化1-范数
    \[\arg\min\lambda\norm{x}_1+\frac{1}{2\alpha}\norm{x-\hat{x}}^2\]
    Hessian矩阵是单位阵，好解得多
    \[\lambda\sum_{i=1}^n\norm{x_i}+\frac{1}{2\alpha}\sum_{i=1}^n(x_i-\hat{x}_i)^2\]
    由于每一维并没有耦合在一起，因此相当于每一个维度都最优化
    \[\arg\min\lambda |x_i|+\frac{1}{2\alpha}(x_i-\hat{x}_i)^2\] % 考试！
    \[0\in\lambda\partial|x_i|+\frac{1}{\alpha}(x_i-\hat{x}_i)\]
    \begin{enumerate}
        \item 若$x_i>0$，则
        \[0=\lambda+\frac{1}{\alpha}(x_i-\hat{x}_i)\]
        \[\implies x_i=\hat{x}_i-\alpha\lambda,\hat{x}_i>\alpha\lambda\]
        \item 若$x_i<0$，则
        \[0=-\lambda+\frac{1}{\alpha}(x_i-\hat{x}_i)\]
        \[\implies x_i=\hat{x}_i+\alpha\lambda,\hat{x}_i<\alpha\lambda\]
        \item 若$x_i=0$，则
        \[0\in[-\lambda,\lambda]-\frac{1}{\alpha}\hat{x}_i\implies 0\in[-\lambda-\frac{1}{\alpha}\hat{x}_i,\lambda-\frac{1}{\alpha}\hat{x}_i]\]
        \[0\geq -\lambda-\frac{1}{\alpha}\hat{x}_i\implies \hat{x}_i\geq -\lambda\alpha,\hat{x}_i\leq\lambda\alpha\]
    \end{enumerate}
    画软门限(soft thresholding)图，横轴$\hat{x}_i$，纵轴$x_i$
    \begin{itemize}
        \item $\iter{x}{k+\frac{1}{2}}=\iter{x}{k}-\alpha A^\T (A\iter{x}{k}-y)$
        \item $\iter{x}{k}=\arg\min_x\lambda\norm{x}_1+\frac{1}{2\alpha}\norm{x-\iter{x}{\frac{1}{2}}}^2$
    \end{itemize}
    即ISTA算法
\end{analysis}

\begin{example}[盒限制(box constrained)优化问题]
\[f_0(x)=s(x)+\sum_{i=1}^nI(x_i\in[l_i,u_i])\]
\end{example}
\begin{analysis}
\[\begin{aligned}
    \qquad\arg\min r(x)+\frac{1}{2\alpha}\norm{x-\hat{x}}^2\\
    &=\arg\min\frac{1}{2\alpha}\norm{x-\hat{x}}^2\\
    &\text{s.t.}\forall i, x_i\in[l_i,u_i]
\end{aligned}\]
如果有约束$x\in\sC$
\begin{itemize}
    \item $\iter{x}{k+\frac{1}{2}}=\iter{x}{k}-\alpha\nabla S(\iter{x}{k})$
    \item $\iter{x}{k+1}=\arg\min_x I_\sC(\iter{x}{k+\frac{1}{2}})+\frac{1}{2\alpha}\norm{x-\iter{x}{k+\frac{1}{2}}}^2=\arg\min_x\frac{1}{2\alpha}\norm{x-\iter{x}{k+\frac{1}{2}}}^2,x\in\sC$
    \item 相当于做投影，故称投影梯度法
\end{itemize}
\end{analysis}

\[\iter{x}{k+\frac{1}{2}}=\iter{x}{k}-\alpha\nabla S(\iter{x}{k})\]
\[\iter{x}{k}=\arg\min_x r(x)+\frac{1}{2\alpha}\norm{x-\iter{x}{k+\frac{1}{2}}}^2\]
\[\begin{aligned}
    0&\in\partial r(\iter{x}{k+1})+\frac{1}{\alpha}(\iter{x}{k+1}-\iter{x}{k+\frac{1}{2}})\\
    0&=\partial r(\iter{x}{k+1})+\frac{1}{\alpha}(\iter{x}{k+1}-\iter{x}{k}+\alpha\nabla S(\iter{x}{k}))\\
    \iter{x}{k+1}&=\iter{x}{k}-\alpha\nabla S(\iter{x}{k})-\alpha\partial r(\iter{x}{k+1})
\end{aligned}\]
数值计算：显式方法（次梯度法）$\to$隐式方法（邻近点梯度法---需要先知道下一步信息，但是这是可以做的，因为有邻近点）

收敛性能与梯度下降法类似

\begin{example}[矩阵补全]
    $Y\in\rr^{m\times n},\{Y_{ij},(i,j)\in\Omega\}$
    \[\min_B\frac{1}{2}\sum_{(i,j)\in\Omega}(Y_{ij}-B_{ij})^2+\lambda\oprank(B)\]
\end{example}
\begin{analysis}
    同LASSO，由于矩阵的秩（奇异值向量0-范数）不好求，改为矩阵的和范数$\norm{\cdot}$（奇异值向量1-范数），即
    \[\min_B\frac{1}{2}\sum_{(i,j)\in\Omega}(Y_{ij}-B_{ij})^2+\lambda\norm{B}_\star\]
    \[\norm{B}_\star:=\sum_{i=1}^n\sigma_i(B)\]
    \[\min\frac{1}{2}\norm{P_\Omega(B-Y)}_F^2+\lambda\norm{B}_\star\]
    其中$P_\Omega$为$0$，若原矩阵中该项不存在；存在的话则保持不变
    \[\nabla B(\frac{1}{2}\norm{P_\Omega}(B-Y)_F^2)=P_\Omega(B-Y)\]
    对$B$做奇异值分解，$U$为酉矩阵
    \[\partial\norm{B}_\star=\{UDV^\T, B=U\Sigma V^\T, d=\partial\norm{\sigma}_1\}\]
    \begin{itemize}
        \item $\iter{B}{k+\frac{1}{2}}=B^k-\alpha P_\Omega(B^k-Y)$
        \item $\iter{B}{k+1}=\arg\min_B\lambda\norm{B}_\star+\frac{1}{2\alpha}\norm{B-\iter{B}{k+\frac{1}{2}}}_F^2$
    \end{itemize}
    \[0\in\lambda\partial\norm{B}_\star+\frac{1}{\alpha}(B-\iter{B}{k+\frac{1}{2}})\]
    \[0\in[\lambda UDV^\T+\frac{1}{\alpha}(B-\iter{B}{k+\frac{1}{2}})]\]
    \[B=U\Sigma V^\T,\;d=\partial\norm{\sigma}_1\]
    \[0\in\{\lambda UDV^\T +\frac{1}{\alpha}(V\Sigma V^\T-\iter{B}{k+\frac{1}{2}})\}\]
    \[\exists V,0=\alpha\lambda UDV^\T+V\Sigma V^\T-\iter{B}{k+\frac{1}{2}}\]
    \[\iter{B}{k+\frac{1}{2}}=U(\alpha\lambda D+\Sigma)V^\T\]
    对$\iter{B}{k+\frac{1}{2}}$进行奇异值分解
    \[\iter{B}{k+\frac{1}{2}}=U\iter{\Sigma}{k+\frac{1}{2}}V\]
    \[T_i=\alpha\lambda d_i+\sigma_i\]
    若$\sigma_i\ne 0\implies \tau_i=\alpha\lambda+\sigma_i$\\
    若$\sigma_i=0\implies \tau_i\in[-\alpha\lambda+\sigma_i,\alpha\lambda+\sigma_i]$\\
    \[\begin{cases}
        \sigma_i=\tau_i-\alpha\lambda & \tau_i>\alpha\lambda\\
        \sigma_i=0 & \tau_i\leq\alpha\lambda
    \end{cases}\]
    该算法称为矩阵软门限(soft thresholding)算法
\end{analysis}

一阶方法总结：
\begin{itemize}
    \item 梯度下降法
    \item 次梯度法：在随机/不确定性优化问题中很有效
    \item 邻近点梯度法
\end{itemize}

\subsection{二阶优化方法}
\subsubsection{牛顿法}
牛顿法(Newton's method)：要求$f_0(x)$二阶可微，强凸
\[\min f_0(\iter{x}{k}+v)\approx\min_{\norm{v}=1}f_0(\iter{x}{k})+\lrang{\nabla f_0(\iter{x}{k}),v}\]
\[\begin{aligned}
    &\approx\min_v f_0(\iter{x}{k})+\lrang{\nabla f_0(\iter{x}{k}),v}+\frac{1}{2}v^\T \nabla^2f(\iter{x}{k})v\\
    &\implies \nabla f_(\iter{x}{k})+\nabla^2f(\iter{x}{k})v=0\\
    &\implies v=-(\nabla^2f(\iter{x}{k}))^{-1}\nabla f_0(\iter{x}{k})\to\text{牛顿方向}
\end{aligned}\]
\[\begin{aligned}
    \iter{d}{k}&=-(\nabla^2f(\iter{x}{k}))^{-1}\nabla f_0(\iter{x}{k})\\
    \iter{x}{k+1}&=\iter{x}{k}-\iter{\alpha}{k}(\nabla f(\iter{x}{k}))^{-1}\nabla f_0(\iter{x}{k})
\end{aligned}\]
其中$\iter{\alpha}{k}$为搜索步长。
看下降方向只要看其与负梯度方向是否小于90度
\[\begin{aligned}
    \qquad& -\nabla f_0(\iter{x}{k}),-(\nabla^2 f_0(\iter{x}{k}))^{-1}\nabla f_0(\iter{x}{k})\\
    =&\nabla^\T f_0(\iter{x}{k})(\nabla^2 f_0(\iter{x}{k}))^{-1}\nabla f_0(\iter{x}{k})
\end{aligned}\]

假设$\nabla^2f(x)$Lipschitz连续
\begin{itemize}
    \item 若$\norm{\nabla f_0(\iter{x}{k})}_2>\eta$，阻尼(damped)牛顿段\\
用Armijo Rule算步长，$\exists\gamma>0$，$f_0(\iter{x}{k+1})-f_0(\iter{x}{k})\leq-\gamma$
    \item 若$\norm{\nabla f_0(\iter{x}{k})}_2\leq\eta$，纯牛顿段\\
$\alpha=1,f_0(\iter{x}{k+1})-f_0(x^\star)\leq\Delta(\frac{1}{2})^{2^k},\exists\Delta>0$，超线性收敛
\end{itemize}
多了二阶信息，往最优解跑的速度会越来越快

\begin{example}
    $\min\frac{1}{2}x^\T px+q^\T r+c,P\succ 0$
\end{example}
\begin{analysis}
    对于二阶强凸问题，只需1步到达最优解；但用梯度下降法，与条件数相关
\end{analysis}
与Newton-Raphson算法的联系，将其扩展至高维的凸问题
\[\iter{x}{k+1}=\iter{x}{k}-\frac{g(\iter{x}{k})}{g'(\iter{x}{k})}\]

\subsubsection{拟牛顿法}
拟牛顿法(quasi-Newton)：希望像一阶算法一样好算，又像二阶算法一样收敛快
\begin{enumerate}
    \item 构造$(\nabla^2f_0(\iter{x}{k}))^{-1}$的近似矩阵$\iter{G}{k}$（直接的想法）
    \item 构造$\nabla^2f_0(\iter{x}{k})$的近似矩阵$\iter{B}{k}$，且容易求逆
\end{enumerate}
在$x=k+1$点处做Taylor展开
\[\begin{aligned}
    \qquad & f_0(x)\approx\nabla f_0(\iter{x}{k+1})+\lrang{\nabla f_0(\iter{x}{k+1}),x-\iter{x}{k+1}}+\frac{1}{2}(x-\iter{x}{k+1})^\T\nabla^2 f_0(\iter{x}{k+1})(x-\iter{x}{k+1})\\
    \implies&\nabla f_0(x)\approx\nabla f_0(\iter{x}{k+1})+\nabla^2 f_0(\iter{x}{k+1})(x-\iter{x}{k+1})\\
    \implies&\nabla f_0(\iter{x}{k})\approx\nabla f_0(\iter{x}{k+1})+\nabla^2 f_0(\iter{x}{k+1})(\iter{x}{k}-\iter{x}{k+1})\\
    \implies&\nabla f_0(\iter{x}{k})-\nabla f_0(\iter{x}{k+1})\approx\nabla^2 f_0(\iter{x}{k+1})(\iter{x}{k}-\iter{x}{k+1})
\end{aligned}\]
\[\begin{aligned}
    \iter{q}{k}&=\nabla f_0(\iter{x}{k+1})-\nabla f_0(\iter{x}{k})\\
    \iter{p}{k}&=\iter{x}{k+1}-\iter{x}{k}
\end{aligned}\]
\[\begin{cases}
    \iter{q}{k}=\iter{B}{k+1}\iter{p}{k}\\
    \iter{p}{k}=\iter{G}{k+1}\iter{q}{k}
\end{cases}\]
\begin{enumerate}
\item 近似$\iter{G}{k}$
\[\iter{G}{k+1}=\iter{G}{k}+\Delta\iter{G}{k}\]
\begin{itemize}
    \item [a.]秩1校正（希望$G$中不要有太多元素，故用低秩矩阵做近似）
    \[\Delta\iter{G}{k}=\iter{\alpha}{k}\iter{z}{k}(\iter{z}{k})^\T\]
    \[\iter{p}{k}=\iter{G}{k+1}\iter{q}{k}=\iter{G}{k}\iter{q}{k}+\iter{\alpha}{k}\iter{z}{k}(\iter{z}{k})^\T\iter{q}{k}\]
    \[\implies\Delta\iter{G}{k}=\frac{(\iter{p}{k}-\iter{G}{k}\iter{q}{k})(\iter{p}{k}-\iter{G}{k}\iter{q}{k})^\T}{(\iter{q}{k})^\T(\iter{p}{k}-\iter{G}{k}\iter{q}{k})}\]
    稳定性很有问题，分母接近0的时候，越接近最优解越不稳定
    \item [b.]秩2校正(Dandon-Fletcher-Power, DFP)
    \[\Delta\iter{G}{k}=\frac{\iter{p}{k}(\iter{p}{k})^\T}{(\iter{p}{k})^\T\iter{q}{k}}-\frac{\iter{G}{k}\iter{q}{k}(\iter{q}{k})^\T\iter{G}{k}}{(\iter{q}{k})^\T\iter{G}{k}\iter{q}{k}}\]
    前后项都为秩1矩阵，数值稳定性强
\end{itemize}
\item 近似$\iter{B}{k}$(Broyden-Fletcher-Goldfarb-Shermo, BFGS)
\[\iter{B}{k+1}=\iter{B}{k}+\frac{\iter{q}{k}(\iter{q}{k})^\T}{(\iter{q}{k})^\T\iter{p}{k}}-\frac{\iter{B}{k}\iter{p}{k}(\iter{p}{k})^\T\iter{B}{k}}{(\iter{p}{k})^\T\iter{B}{k}\iter{p}{k}}\]
\end{enumerate}
\begin{itemize}
    \item 拟牛顿法以后可能很有用，因为结合一二阶优化优点
    \item 找核心问题（Hessian矩阵难算），然后就去解决
    \item 用\textbf{结构信息}，都对结构进行限制（一股脑就用Adam优化器，这是不对的，要分析问题结构）
\end{itemize}

有限内存(limited memory)---LM-BFGS

\subsection{有约束优化方法}
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{x}{\in C}
\end{mini*}
变为
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{A x}{=b}
\end{mini*}
本质上都是在考虑它的KKT条件
\[\begin{cases}
    A x^\star=b\\
    \nabla f_0(x^\star)+A^\T v^\star=0
\end{cases}\]
\begin{mini*}
    {}{\frac{1}{2}x^\T px+q^\T x+r,P\succeq 0}{}{}
    \addConstraint{Ax}{=b}
\end{mini*}
等价于KKT条件
\[\begin{cases}
    A x^\star=b\\
    p x^\star+q+A^\T v^\star=0
\end{cases}\]
等价于
\[\bmat{P & A^\T\\A & \vzero}\bmat{x^\star \\v^\star}=\bmat{-q\\b}\]

\subsubsection{约束满足的牛顿法}
若方程组非线性，那就做一个线性化
\begin{argmini*}
{d}{f_0(\iter{x}{k}+d)=\iter{d}{k}}{}{}
\addConstraint{A(\iter{x}{k}+d)}{=b}
\end{argmini*}
近似等价于（二阶近似Taylor展开）
\begin{argmini*}
{d}{f_0(\iter{x}{k})+\lrang{\nabla f_0(\iter{x}{k}),d}+\frac{1}{2}d^\T\nabla^\T f_0(\iter{x}{k})d=\iter{d}{k}}{}{}
\addConstraint{A(\iter{x}{k}+d)}{=b}
\end{argmini*}
写出问题关于d的KKT条件，可得等价条件
\[\bmat{\nabla f_0(\iter{x}{k}) & A^\T\\ A & \vzero}\bmat{\iter{d}{k}\\\iter{v}{k}}=\bmat{-\nabla f_0(\iter{x}{k})\\b-A\iter{x}{k}}\]
若$\iter{x}{0}$可行，$A\iter{x}{0}=b$，之后的$\iter{x}{k+1}=\iter{x}{k}+\iter{\alpha}{k}\iter{d}{k}$也可行。
即为。

\subsubsection{拉格朗日乘子法/对偶分解法}
\[L(x,v)=f_0(x)+\lrang{Ax-b,v}\]
更新原变量和对偶变量
\[\begin{cases}
    \iter{x}{k+1}=\arg\min_x L(x,\iter{v}{k})\\
    \iter{v}{k+1}=\iter{v}{k}+\iter{\alpha}{k}(A\iter{x}{k+1}-b)
\end{cases}\]
$\iter{\alpha}{k}$可以是固定步长，也可以是递减步长

即为\textbf{找鞍点}，$x$方向上找最小值，本来$v$方向上要找最大值，但容易到正无穷。因此换种方法$\iter{v}{k}$做一个保守的计算，每一步都走一个很小的步长。

\begin{example}
    \begin{mini*}
        {}{\frac{1}{2}x^2}{}{}
        \addConstraint{x}{=1}
    \end{mini*}
\end{example}
\begin{analysis}
    \[\begin{aligned}
        L(x,v)&=\frac{1}{2}x^2+v(x-1)\\
        &=\frac{1}{2}x^2+vx-v
    \end{aligned}\]
\end{analysis}

\bigskip
\textbf{对偶次梯度法}：$v$才是最关键的，只是在寻找最优$v$的时候顺带找到了$x$（收敛到$v^\star$的同时也找到了$x^\star$）
\[D(v)=\inf_x L(x,v)\]
$D(v)$为凹函数，关注$-D(v)$
\[\begin{cases}
    -(A\iter{x}{k+1}-b)\\
    \iter{x}{k+1}=\arg\min_x L(x,\iter{v}{k})
\end{cases}\]
\[\iter{v}{k+1}=\iter{v}{k}-\iter{\alpha}{k}(-(A\iter{x}{k+1})-b)\]

若$f_0(x)$为凸，若$\hat{x}=\arg\min_x L(x,\hat{\lambda})$，则$-(A\hat{x}-b)$为$-D(\lambda)$在$\hat{\lambda}$的次梯度
\[\forall v:\;-D(\lambda)\geq -D(\hat{\lambda})+\lrang{v-\hat{\lambda},g(\hat{\lambda})}\]
\[\begin{aligned}
    D(v)&=\inf_x L(x,v)\\
    &=\inf_x f_0(x)+\lrang{v,Ax+b}\\
    &\leq f_0(\hat{x})+\lrang{v,A\hat{x}-b}\\
    &=f_0(\hat{x})+\lrang{\lambda,A\hat{x}-b}+\lrang{v-\hat{\lambda},A\hat{x}-b}\\
    &=D(\hat{\lambda})+\lrang{v-\hat{\lambda},A\hat{x}-b}
\end{aligned}\]
\[-D(v)\geq -D(\hat{\lambda})+\lrang{v-\hat{\lambda},-(A\hat{x}-b)}\]
进而得到$-(A\hat{x}-b)$就是一个次梯度

这个算法一般来说性能不好，在机器学习里面很多时候都被乱用，有时候可以，有时候不行。
\par 在什么情况下它是好用的？对偶函数是可微的，采用固定步长。
\par 对偶函数$D(v)$何时可微？

任何$-D(\lambda)$都具有$-(A\hat{x}-b)$的形式，得到当$f_0(x)$\textbf{严格凸}时，$f_0(x)+\lrang{\hat{\lambda},Ax-b}$严格凸，进而$D(v)$可微

\bigskip
\textbf{原对偶次梯度法}：计算量出在$\iter{x}{k+1}$，那么想办法近似
\[\begin{cases}
    \iter{x}{k+1}&=\iter{x}{k}-\iter{\alpha}{k}\partial_x L(x,\iter{v}{k})\\
    \iter{v}{k+1}&=\iter{v}{k}+\iter{\alpha}{k}\partial_v L(\iter{x}{k+1},v)\\
    &\iter{v}{k}+\iter{\alpha}{k}(A\iter{x}{k+1}-b)
\end{cases}\]

$\iter{v}{k+1}$需要等待$\iter{x}{k+1}$，将其换成下式可以不用等待
\[\iter{v}{k}+\iter{\alpha}{k}(A\iter{x}{k}-b)\]

由于两个方向都不精确，故收敛性质糟糕。

\subsubsection{增广(augmented)拉格朗日法}：当函数不是严格凸时，依然能得到很好的效果
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{Ax}{=b}
\end{mini*}
\[L(x,v)=f_0(x)+\lrang{v,Ax-b}\]
\[L_c(x,v)=f_0(x)+\lrang{v,Ax-b}+\frac{c}{2}\norm{Ax-b}^2,c>0\]
增广拉格朗日函数是另一个问题的拉格朗日函数
\begin{mini*}
    {}{f_0(x)+\frac{c}{2}\norm{Ax-b}^2}{}{}
    \addConstraint{Ax}{=b}
\end{mini*}
两个问题的原对偶最优解相同

设$(x^\star,v^\star)$为原问题最优解
\[\begin{cases}
    Ax^\star=b\\
    \pd{L(x,v^\star)}{x}\Big|_{x=x^\star}=0
\end{cases}\]
\[\begin{cases}
    Ax^\star=b\\
    \pd{L_c(x,v^\star)}{x}\Big|_{x=x^\star}=0
\end{cases}\]
\[\nabla_x)(f_0(x)+\lrang{v^\star,Ax-b})\Big|_{x=x^\star}=0\]
\[\begin{aligned}
    \qquad&\nabla_x)(f_0(x)+\lrang{v^\star,Ax-b}+\frac{c}{2}\norm{Ax-b}^2)\Big|_{x=x^\star}=0\\
    =&\nabla_x(\frac{c}{2}\norm{Ax-b}^2)\\
    =&cA^\T(Ax-b)\Big|_{x=x^\star}=0
\end{aligned}\]

\[\begin{cases}
    \iter{x}{k+1}=\arg\min_x L_c(x,\iter{v}{k})\\
    \iter{v}{k+1}=\iter{v}{k}+c(A\iter{x}{k+1}-b)
\end{cases}\]
只要原问题是凸问题，无论$c$怎么取（$c$刚好就是固定步长），该算法总是可以收敛（不考虑计算精度的问题），只是收敛速度不同

% 考试必考手算原变量和对偶变量
\begin{example}
    \begin{mini*}
        {}{\frac{1}{2}x_1^2+\frac{1}{2}x_2^2}{}{}
        \addConstraint{x_1}{=1}
    \end{mini*}
\end{example}
\begin{analysis}
    \[L(x,v)=\frac{1}{2}x_1^2+\frac{1}{2}x_2^2+v(x_1-1)\]
    \[\pd{L(x,v^\star)}{x}\Big|_{x=x^\star}=0=\bmat{x_1+v^\star\\x_2}\]
    增广拉格朗日法
    \[L_{c^k}(x,v)=\frac{1}{2}x_1^2+\frac{1}{2}x_2^2+v(x_1-1)+\frac{c^k}{2}(x_1-1)^2\]
    \[\iter{x}{k+1}=\arg\min L_{c^k}(x,\iter{v}{k})\]
    \[\begin{cases}
        x_1+\iter{v}{k}+\iter{c}{k}(x_1-1)=0\\
        x_2=0
    \end{cases}\]
    \[\iter{x}{k+1}=\bmat{\frac{\iter{c}{k}-\iter{v}{k}}{\iter{c}{k}+1}\\0}\]
    \[\begin{aligned}
        \iter{v}{k+1}&=\iter{v}{k}+\iter{c}{k}(\iter{x_1}{k+1}-1)\\
        &=\iter{v}{k}+\iter{c}{k}\lrp{\frac{\iter{c}{k}-\iter{v}{k}}{\iter{c}{k}+1}-1}\\
        &=\frac{\iter{v}{k}}{\iter{c}{k}+1}-\frac{\iter{c}{k}}{\iter{c}{k}+1}
    \end{aligned}\]
    \[\iter{v}{k+1}-v^\star=\iter{v}{k+1}+1=\frac{\iter{v}{k}}{\iter{c}{k}+1}+\frac{1}{\iter{c}{k}+1}=\frac{\iter{v}{k}-v^\star}{\iter{c}{k}+1}\]
    可以看出取一个固定步长，且大于零，增广拉格朗日的收敛是非常好的（线性收敛）\\
    对于特殊的一些非凸问题，增广拉格朗日也是有效的，如把问题改成
    \[\min -\frac{1}{2}x_1^2+\frac{1}{2}x_2^2\]
\end{analysis}

\subsubsection{交替方向乘子法}
交替方向乘子法(alternating direction method of multipliers, ADMM)同样探究\textbf{有结构}的优化问题。
\begin{mini*}
    {}{f_1(x)+f_2(x)}{}{}
    \addConstraint{Ax+By}{=0}
\end{mini*}
\[L_c(x,y,v)=f_1(x)+f_2(y)+\lrang{v,Ax+By}+\frac{c}{2}\norm{Ax+By}_2^2\]
\[\begin{cases}
    (\iter{x}{k+1},\iter{y}{k+1})=\arg\min_{x,y}L_c(x,y,\iter{v}{k})\\
    \iter{v}{k+1}=\iter{v}{k}+c(A\iter{x}{k+1}+B\iter{y}{k+1})
\end{cases}\]
由于在$\norm{Ax+By}_2^2$中，$x$和$y$结合在一起，不好优化，故用交替的方法（选主元）来解决
\[\begin{cases}
    \iter{x}{k+1}=\arg\min_{x}L_c(x,\iter{y}{k},\iter{v}{k})\\
    \qquad=\arg\min_x f_1(x)+\lrang{\iter{v}{k},Ax}+\frac{c}{2}\norm{Ax+B\iter{y}{k}}_2^2\\
    \qquad\iff\arg\min_x f_1(x)+\frac{c}{2}\norm{Ax+B\iter{y}{k}+\frac{\iter{v}{k}}{c}}_2^2\qquad\text{配方，关于$\iter{y}{k}$的项为常数项，可忽略}\\
    \iter{y}{k+1}=\arg\min_{y}L_c(\iter{x}{k+1},y,\iter{v}{k})\\
    \qquad\iff\arg\min_y f_1(x)+\frac{c}{2}\norm{A\iter{x}{k+1}+By+\frac{\iter{v}{k}}{c}}_2^2\\
    \iter{v}{k+1}=\iter{v}{k}+c(A\iter{x}{k+1}+B\iter{y}{k+1})
\end{cases}\]
两块的算法依然具有很好的收敛性，但是多块的交替方向乘子法不一定可以收敛。

\begin{example}[LASSO]
    \[\min\frac{1}{2}\norm{Ax-b}_2^2+v\norm{x}_1\]
\end{example}
\begin{analysis}
    写成交替方向乘子法的格式
    \begin{mini*}
        {}{\frac{1}{2}\norm{Ax-b}_2^2+v\norm{y}_1}{}{}
        \addConstraint{x-y}{=0}
    \end{mini*}
    \[\begin{cases}
        \iter{x}{k+1}=\arg\min_x\frac{1}{2}\norm{Ax-b}_2^2+\frac{c}{2}\norm{x-\iter{y}{k}+\frac{\iter{v}{k}}{c}}_2^2\\
        \iter{x}{k+1}=\arg\min_y v\norm{y}_1+\frac{c}{2}\norm{\iter{x}{k+1}-y+\frac{\iter{v}{k}}{c}}_2^2\\
        \iter{v}{k+1}=\iter{v}{k}+c(\iter{x}{k+1}-\iter{y}{k+1})
    \end{cases}\]
    对于$x$可以求出显式解
    \[\begin{aligned}
        &\qquad A^\T(Ax-b)+c(x-\iter{y}{k}+\frac{\iter{v}{k}}{c})\\
        &\implies (A^\T A+cI)x=A^\T b+c\iter{y}{k}-\iter{v}{k}\\
    \end{aligned}\]
    同样对于$y$也可以求出显式解
\end{analysis}

\subsubsection{并行优化}
\begin{center}
    \begin{tikzcd}
        & \text{master }g(x)\arrow{dl}\arrow{d}\arrow{dr} & \\
        \text{worker1 }f_1(x) & \text{worker2 }f_2(x) & \text{worker3 }f_3(x)
    \end{tikzcd}
\end{center}
\[\min_x g(x)+\sum_{i=1}^N f_i(x)\]
针对LASSO问题，每个人都有一个样本$(A_i,b_i)$，最小化样本之和，以及正则化项
\[\begin{cases}
    (A_1,b_1)\implies\frac{1}{2}\norm{A_1 x-b_1}_2^2\\
    \vdots\\
    (A_n,b_n)\implies\frac{1}{2}\norm{A_N x-b_N}_2^2\\
    g(x)=v\norm{x}_1
\end{cases}\]
原问题即为
\[\min_x v\norm{x}_1+\frac{c}{2}\norm{\bmat{A_1 & \cdots & A_N}x-\bmat{b_1 & \cdots & b_N}}_2^2\]

\textbf{并行梯度下降法}
\[\begin{cases}
    \iter{x}{k+\frac{1}{2}}=\iter{x}{k}-\alpha\sum_{i=1}^N\nabla f_i(\iter{x}{k})\\
    \iter{x}{k+1}=\arg\min g(x)+\frac{1}{2\alpha}\norm{x-\iter{x}{k+\frac{1}{2}}}_2^2
\end{cases}\]
计算简单，只需求梯度，但所有梯度类问题都依赖于条件数。通信开销大。

\textbf{对偶分解法}
\begin{mini*}
    {}{\sum_{i=1}^N f_i(x_i)+g(z)}{}{}
    \addConstraint{x_i}{=z,\forall i}
\end{mini*}
\[L(x,z,v)=\sum_{i=1}^N f_i(x_i)+g(z)+\sum_{i=1}^N\lrang{v_i,x_i z}\]
\[(\iter{x}{k+1},\iter{z}{k+1})=\arg\min_{x,z}L(x,z,\iter{v}{k})\]
\[\implies\begin{cases}
    \iter{x_i}{k+1}=\arg\min_{x_i} f_i(x_i)+\lrang{\iter{v_i}{k},x_i}\\
    \iter{z}{k+1}=\arg\min_{z_i} g(z)-\sum_{i=1}^N\lrang{\iter{v_i}{k},z}\\
    \iter{v_i}{k+1}=\iter{v_i}{k}+\alpha\lrang{\iter{x_i}{k+1},\iter{z}{k+1}}
\end{cases}\]
不依赖于条件数，但每一步都需要求解一个最优解，不一定好求。通信开销小，但拉格朗日类方法收敛性差。

\textbf{增广拉格朗日函数}
\[\sum_{i=1}^nf_i(x_i)+g(z)+\sum_{i=1}^n\lrang{\lambda_i,x-z}+\frac{c}{2}\sum_{i=1}^n\norm{x_i-z}^2\]
正则项会产生$x_i$和$z$的交叉项，不好处理

注意到$x_i$之间是没有依赖的，故采用交替方向乘子法，加了增广项，可用固定步长达到最优解
\[\begin{cases}
    \iter{x_i}{k+1}&=\arg\min f_0(x_i)+\lrang{\lambda_i^k,x_i}+\frac{c}{2}\norm{x_i-\iter{z}{k}}^2\\
    \iter{z}{k+1}&=\arg\min g(z)-\lrang{\sum_{i=1}^n\iter{\lambda_i}{k},z}+\frac{c}{2}\sum_{i=1}^n\norm{\iter{x_i}{k+1}-z}^2\\
    \iter{\lambda}{k+1}&=\iter{\lambda}{k}+c(\iter{x_i}{k+1}-\iter{z}{k+1})
\end{cases}\]
每次要多传一倍的变量，以通信量开销换性能提升

\textbf{无中心分布式优化}
考虑无向图
\begin{center}
    \begin{tikzcd}
        & o\arrow{r}\arrow{ld}\arrow{rd} & o\arrow{d}\\
        o\arrow{rd} & & o\arrow{ld}\arrow{d}\\
        & o\arrow{uu} & o\arrow{l}
    \end{tikzcd}
\end{center}
每个结点自己优化，协同决策
\[\min\sum_{i=1}^n f_i(x)\]
梯度下降法，但由于去中心，$\iter{x}{k}$无处摆放
\[\iter{x}{k+1}=\iter{x}{k}-\iter{\alpha}{k}\sum_{i=1}^n\nabla f_i(\iter{x}{k})\]
那就每一个点分配一个本地变量$x_i$，对邻居的更新做一个加权平均
\[\iter{x}{k+1}=\sum_{i=1}^n\omega_{ij}\iter{x_j}{k}-\iter{\alpha}{k}\sum_{i=1}^n\omega_{ij}\nabla f_j(\iter{x_j}{k})\]
其中
\[\begin{cases}
    \omega_{ij}\ne 0 & (i,j)\in E, i=j\\
    \omega_{ij}=0 & \text{otherwise}
\end{cases}\]
\[W=\bmat{\omega_{ij}},W=W^\T,W\vone=\vone\]

在不可信的系统里面，存在个人隐私等信息，故更激进些，采用自己的梯度进行更新（在本地进行梯度下降），在无人机系统中非常常见
\[\iter{x_i}{k+1}=\sum_{i=1}\omega_{ij}\iter{x_j}{k}-\iter{\alpha}{k}\nabla f_i(\iter{x_i}{k})\]
非常糟糕的算法，如果采用固定步长，则找不到最优解
\begin{analysis}
反证法，假设$\iter{x_i}{k}\to x^\star$，将$x^\star$代入
\[x^\star=\sum_{j=1}^n\omega_{ij}x^\star-\alpha\nabla f_i(x^\star)\iff \nabla f_i(x^\star)=0\]
但原问题最优解
\[\sum_{i=1}^n\nabla f_i(x^\star)=0\]
与上面的式子不等价
\end{analysis}

改成有约束优化的形式
\begin{mini*}
    {}{\sum_{i=1}^n f_i(x_i)}{}{}
    \addConstraint{x_1=x_2=\cdots=x_n}
\end{mini*}
写出拉格朗日函数，对偶分解法
\[\sum_{i=1}^nf_i(x_i)+\sum_{(i,j)\in E}\lrang{\lambda_{ij},x_i-x_j}\]
别人的东西都在对偶变量中体现，求同存异
\[\begin{cases}
    \iter{x_i}{k+1}=\arg\min f_i(x_i)+\sum_{(i,j)\in E}(\iter{\lambda_{ij}}{k},x_i)-\sum_{(j,i)\in E}\lrang{\lambda_{ij},x_i}\\
    \iter{\lambda_{ij}}{k+1}=\iter{\lambda_{ij}}{k}+\iter{\alpha}{k}(\iter{x_i}{k+1}-\iter{x_j}{k+1})-\sum_{(j,i)\in E}\lrang{\iter{\lambda_{ij}},x_i}
\end{cases}\]
依然要采用递减步长，才能保证收敛

\begin{center}
    \begin{tikzcd}
        x_i\arrow[bend left]{rr}{z_{ij}} & & x_j\arrow[bend left]{ll}{z_{ji}}
    \end{tikzcd}
\end{center}
\begin{mini*}
    {}{\sum_{i=1}^n f_i(x_i)}{}{}
    \addConstraint{x_i}{=z_{ij},\forall (i,j)\in E}
    \addConstraint{x_j}{=z_{ji},\forall (i,j)\in E}
\end{mini*}
可分线性约束，进而可以用交替方向乘子法

\[\begin{aligned}
    \sum_{i=1}^nf_i(x_i)&+\sum_{(i,j)\in E}\lrp{\lrang{\alpha_{ij},x_i-z_j}+\lrang{\beta_{ij},x_j-z_{ji}}}\\
    &+\sum_{(i,j)\in E}\frac{c}{2}\lrp{\norm{x_i-z_{ij}}^2+\norm{x_j-z_{ij}}^2}
\end{aligned}\]
\[\begin{cases}
    \iter{x_i}{k+1}&=\arg\min f_i(x_i)+\sum_{(i,j)\in E}\lrang{\iter{\alpha_{ij}{k}},x_i}+\sum_{(i,j)\in E}\lrang{\iter{\beta_{ji}}{k},x_i}\\
    \qquad&+\frac{c}{2}\sum_{(i,j)\in E}\norm{x_i-\iter{z_j}{k}}^2+\frac{c}{2}\sum_{(i,j)\in E}\norm{x_i-\iter{z_{ji}}{k}}^2\\
    \iter{z_j}{k+1}&=\cdots\\
    \iter{\alpha_{ij}}{k+1}&=\cdots\\
    \iter{\beta_{ij}}{k+1}&=\cdots
\end{cases}\]