% !TEX root = main.tex

\section{对偶理论}
拉格朗日函数(Lagrangian function)
\[L(x,\lambda,v)=f_0(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^pv_ih_i(x),\dom L=D\times \rr^m\times \rr^p\]
拉格朗日乘子(multiplier)
\begin{itemize}
\item 原变量(primal variable)：$\lambda=\bmat{\lambda_1 & \cdots & \lambda_m}^\T$
\item 对偶变量(dual variable)：$v=\bmat{v_1 & \cdots & v_p}^\T$
\end{itemize}

拉格朗日对偶函数
\[\begin{aligned}
    g(\lambda,v)&=\inf_{x\in\sD} L(x,\lambda,v)\\
    &=\inf_{x\in\sD}\lrp{f_0(x)+\sum_{i=1}^m\lambda_if_i(x)+\sum_{i=1}^pv_ih_i(x)}
\end{aligned}\]
注意遍历域是$\sD=\bigcap_{i=0}^m\dom f_i\cap\bigcap_{i=1}^p\dom h_i$，而不是可行解集$\sX$\\
\begin{itemize}
    \item $g(\lambda,v)$一定是关于$\lambda$和$v$的凹函数（关于$\lambda$和$v$的仿射函数，注意$x$为常数）
    \item $\forall\lambda\geq 0,\forall v,g(\lambda,v)\leq P^\star$\\
    对偶(dual)问题
    \begin{maxi*}
        {}{g(\lambda,v)}{}{}
        \addConstraint{\lambda}{\geq 0}
    \end{maxi*}
    其最优解记为$D^\star$，则$D^\star\leq P^\star$，即给出了原问题的一个最优下界
\end{itemize}

$x^\star$原问题最优解
\[\sum_{i=1}^m\lambda_i f_0(x^\star)+\sum_{i=1}^p v_i h_i(x^\star)\leq 0\]
\[L(x^\star,\lambda,v)=f_0(x^\star)+(\cdots)\leq P^\star\]
\[g(\lambda,v)=\inf_{x\in\sD} L(x,\lambda,v)\leq L(x^\star,\lambda,v)\leq P^\star\]

\begin{example}
\begin{mini*}
    {}{x^\T x}{}{}
    \addConstraint{Ax}{=b}
\end{mini*}
\end{example}
\begin{analysis}
\[L(x,v)=x^\T x+v^\T(Ax-b)\]
\[\begin{aligned}
    g(v)&=\inf_{x\in\sD} L(x,v)\\
    &=\inf_{x\in\sD} x^\T x+v^\T A x-v^\T b\\
    &=(-\frac{A^\T v}{2})^\T(-\frac{A^\T v}{2})+v^\T A(-\frac{A^\T v}{2})-v^\T b\\
    &=-\frac{1}{4}v^\T AA^\T v-b^\T v
\end{aligned}\]
补充求梯度：$2x+A^\T v=0\implies x=-\frac{A^\T v}{2}$

因而得到对偶问题
\[\max_v-\frac{1}{4}v^\T AA^\T v-b^\T v\]
\end{analysis}

\begin{example}
\begin{mini*}
    {}{c^\T x}{}{}
    \addConstraint{Ax}{=b}
    \addConstraint{x}{\geq 0}
\end{mini*}
\end{example}
\begin{analysis}
    注意$\lambda$前面符号，要化为一般形式
    \[L(x,\lambda,v)=c^\T x-\lambda^\T x+v^\T(Ax-b)\]
    \[\begin{aligned}
        g(\lambda,v)&=\inf_x L(x,\lambda,v)\\
        &=\inf_x(c-\lambda+A^\T v)^\T x-v^\T b\\
        &=\begin{cases}-\infty&c-\lambda+A^\T v\ne 0\\-v^\T b&c-\lambda+A^\T v=0\end{cases}
    \end{aligned}\]
    对偶问题，由于要极大，故不考虑负无穷部分
    \begin{maxi*}
        {\lambda,v}{-v^\T b}{}{}
        \addConstraint{c-\lambda+A^\T v}{=0}
        \addConstraint{\lambda}{\geq 0}
    \end{maxi*}
    逆过来求解
    \begin{mini*}
        {}{b^\T v}{}{}
        \addConstraint{A^\T v+c}{\geq 0}
    \end{mini*}
    \[L(v,\lambda)=b^\T v-\lambda^\T(A^\T v+c)\]
    \[\begin{aligned}
        g(\lambda)&=\inf_v L(v,\lambda)\\
        &=\inf(b-A\lambda)^\T v-\lambda^\T c\\
        &=\begin{cases}
            -\lambda^\T c & b-A\lambda =0\\
            -\infty & b-A\lambda\ne 0
        \end{cases}
    \end{aligned}\]
    \begin{maxi*}
        {}{-\lambda^\T c}{}{}
        \addConstraint{b-A\lambda}{=0}
        \addConstraint{\lambda}{\geq 0}
    \end{maxi*}
    对偶的对偶不一定回去，线性规划才满足
\end{analysis}
% 对偶支撑向量机(SVM) 升变量维度，降约束维度

\begin{example}
\begin{mini*}
    {}{x^\T wx}{}{}
    \addConstraint{x_i}{=\pm 1,}{\quad i=1,\ldots,n}
\end{mini*}
\end{example}
\begin{analysis}
    \[L(x,v)=x^\T wx+\sum_{i=1}^n v_i(x_i^2-1)\]
    \[\begin{aligned}
        g(v)&=\inf_x L(x,v)\\
        &=\inf_x x^\T wx+\sum_{i=1}^n v_ix_i^2-\sum_{i=1}^n v_i\\
        &=\inf_x x^\T\lrp{w+\opdiag v}x-\vone^\T v
        &=\begin{cases}-\vone^\T v & w+\opdiag(v)\succeq 0\\-\infty&\text{otherwise}\end{cases}
    \end{aligned}\]
    补充求梯度：$2(w+\opdiag(v))x=0$
    \begin{maxi*}
        {v}{-\vone^\T v}{}{}
        \addConstraint{w+\opdiag(v)}{\succeq 0}
    \end{maxi*}
\end{analysis}

\begin{definition}[函数的共轭]
    $f:\rn\mapsto\rr,f^\star(y)=\sup_{x\in\dom f}(y^\T x-f(x))$，
    几何意义即到不同斜率直线的距离最大值
\end{definition}
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{Ax}{\leq b}
    \addConstraint{cx}{=d}
\end{mini*}
\[\begin{aligned}
    L(x,\lambda,v)&=f_0(x)+\lambda^\T(Ax-b)+v^\T(cx-d)\\
    &=f_0(x)+(A^\T\lambda+c^\T v)^\T x-\lambda^\T b-v^\T d\\
    g(\lambda,v)&=\inf_x f_0(x)+(A^\T\lambda+c^\T v)^\T x-\lambda^\T b-v^\T d\\
    &=-\sup_x-(A^\T\lambda+c^\T v)^\T x-f_0\\
    &=-f_0^\star(-(A^\T\lambda+c^\T v))-\lambda^\T b-v^\T d
\end{aligned}\]

对偶间隙(duality gap)：$p^\star-d^\star\geq 0$
\begin{itemize}
    \item 弱对偶：严格大于0
    \item 强对偶：对偶间隙为0
\end{itemize}

\begin{enumerate}
    \item 对于非凸问题，\textbf{通常}$p^\star\ne d^\star$
    \item 对于凸问题，若slater条件满足，$p^\star=d^\star$
\end{enumerate}

\begin{definition}[相对内点(relative interior)]
    \[\mathop{relint} D=\{x\in D\mid B(x,r)\cap\opaff D\subset v,\exists r>0\}\]
\end{definition}

\begin{theorem}[Slater条件]
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{f_i(x)}{\leq 0}{,\quad i=1,\ldots,m}
    \addConstraint{Ax}{=b}
\end{mini*}
$\exists x\in\mathop{relint} D$使得$f_i(x)<0,i=1,\ldots,m,Ax=b$
\end{theorem}
\begin{example}
    二次规划(QP)
    \begin{mini*}
        {}{x^\T x}{}{}
        \addConstraint{Ax}{=b}
    \end{mini*}
    Slater条件$\{x\mid Ax=b\}$非空
\end{example}
\begin{example}
    二次约束二次规划(QCQP)
    \begin{mini*}
        {}{\frac{1}{2}x^\T P_0 x+q_0^\T+r_0}{}{}
        \addConstraint{\frac{1}{2}x^\T P_i x+q_i^\T x+r_i}{\leq 0}{,\quad i=1,\ldots,m}
    \end{mini*}
    $P_0,\ldots,P_i$半正定
\end{example}

凸问题+Slater条件$\implies p^\star=d^\star$，但有可能不满足Slater条件也依然强对偶
\begin{example}
\begin{mini*}
    {}{x,x\in\rr}{}{}
    \addConstraint{x}{leq 0}
    \addConstraint{-x}{\leq 0}
\end{mini*}
\end{example}
\begin{analysis}
    \[L(x,\lambda_1,\lambda_2)=x+\lambda_1 x-\lambda_2 x=(1+\lambda_1-\lambda_2)x\]
\[g(\lambda_1,\lambda_2)=\inf_{x\in\rr}(1+\lambda_1-\lambda_2)x=
\begin{cases}0& 1+\lambda_1-\lambda_2=0\\ -\infty &\text{otherwise}\end{cases}\]
    \begin{maxi*}
        {\lambda_1,\lambda_2}{0}{}{}
        \addConstraint{1+\lambda_1-\lambda_2}{=0}
    \end{maxi*}
\[\implies p^\star=d^\star=0\]
\end{analysis}

置信域问题
\begin{mini*}
    {}{x^\T Ax+b^\T x}{}{}
    \addConstraint{x^\T x}{\leq 1}
    \addConstraint{A}{\nsucceq 0}
\end{mini*}
依然可以得到$p^\star=d^\star$

几何解释
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{f_i(x)}{\leq 0}{,\quad i=1,\ldots,m}
\end{mini*}

\[G=\{(f_1(x),f_0(x))\mid x\in\sD\}\]
\[g(\lambda)=\inf\{t+\lambda u\mid(u,t)\in G\}\]
\[L(x,\lambda)=f_0(x)+\lambda f_1(x)\]
\[g(\lambda)=\inf_{x\in\sD}\{f_0(x)+\lambda f_1(x)\}\]

\[p^\star=\inf\{t\mid(u,t)\in G,u\leq 0\}\]
\[\lambda\geq 0,\max g(\lambda)\]

注意问题必须要有可行解

经济学解释：满足原材料约束下，利润最多
价格$\lambda_i\geq 0$
\[g(\lambda)=\inf_x f_(x)+\lambda_1 f_1(x)+\cdots+\lambda_m f_m(x)=\inf_x L(x,\lambda)\]
则$g(\lambda)$为对偶函数，市场$p^\star$损失最小（$g(\lambda)\leq p^\star$）
\[d^\star=\sup_{lambda\geq 0}g(\lambda)\]
市场平衡点，均衡市场$p^\star=d^\star$，最优/影子价格$\lambda^\star$

多目标优化解释
\[\begin{cases}
    \min f_0(x) & 1\\
    \min f_1(x) & \lambda_1\\
    \vdots & \vdots\\
    \min f_m(x) & \lambda_m
\end{cases}\]
\[\min_x f_0(x)+\lambda_1 f_1(x)+\cdots+\lambda_m f_m(x)\]

鞍点(saddle point)解释
\[f(w,z),w\in S_w,z\in S_z\]
极小极大不等式
\[\sup_{z\in S_z}\inf_{w\in S_w} f(wz)\leq \inf_{w\in S_w}\sup_{z\in S_z}f(w,z)\]
若有$(\tilde{w},\tilde{z})$使得
\[\begin{aligned}
    (\tilde{w},\tilde{z})&=\arg\max_{z\in S_z}\min_{w\in S_w} f(w,z)
    (\tilde{w},\tilde{z})&=\arg\min_{w\in S_w}\max_{z\in S_z} f(w,z)
\end{aligned}\]
则$(\tilde{w},\tilde{z})$为鞍点

有下面不等式成立
\[f((\tilde{w},z))\leq f(\tilde{w},\tilde{z})\leq f(w,\tilde{z}),\forall z\in S_z,w\in S_w\]
即从一个方向望过去是最小，从另一个方向望过去是最大

\[\begin{aligned}
    &L(x,\lambda)&=f_0(x)+\sum_{i=1}^m\lambda_i f_i(x)\\
    \implies&\sup_{\lambda\geq 0}L(x,\lambda)&=\sup_{\lambda\geq 0}\{f_0(x)+\sum_{i=1}^m\lambda_if_i(x)\}\\
    &&=\begin{cases}f_0(x)&f_i(x)\leq 0,i=1,\ldots,m\\+\infty&\text{otherwise}\end{cases}\\
    \implies& p^\star&=\inf_x\{f_0(x)\mid f_i(x)\leq 0,i=1,\ldots,m\}=\inf_x\sup_{\lambda\geq 0}L(x,\lambda)
\end{aligned}\]
\[d^\star=\sup_{\lambda\geq 0}g(\lambda)=\sup_{\lambda\geq 0}\inf_x L(x,\lambda)\implies p^\star\geq d^\star\]

如果$L(x,\lambda)$有鞍点，则必有$p^\star=d^\star$

鞍点在无约束优化问题中是很糟糕的点（所有方向上梯度为0），但是有约束优化问题则是非常好的点

若$(\tilde{x},\tilde{\lambda})$为$L(x,\lambda)$鞍点$\iff p^\star=d^\star$且$\tilde{x},\tilde{\lambda}$为原对偶问题最优解
$\implies$若为鞍点，$p^\star=d^\star$
\[\sup_{\lambda\geq 0}\inf_x L(x,\lambda)=\inf_x\sup_{\lambda\geq 0}L(x,\lambda)\]
已知$(\tilde{x},\tilde{\lambda})$为左边最优
\[\begin{aligned}
    \tilde{\lambda}&=\arg\max_{\lambda\geq 0}\inf_x L(x,\lambda)\\
    \tilde{x}&=\arg\inf_x\sup_{\lambda\geq 0}L(x,\lambda)
\end{aligned}\]
则$\tilde{\lambda}$对偶最优，$\tilde{x}$为原问题最优

% 4.4
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{f_i(x)}{\leq 0}{,\quad i=1,\ldots,m}
\end{mini*}
\begin{theorem}
    $(\tilde{x},\tilde{\lambda})$为拉格朗日函数鞍点$\iff p^\star=d^\star$，且$(\tilde{x},\tilde{\lambda})$为原对偶的最优解
\end{theorem}
\begin{analysis}
    右推左，$(\tilde{x},\tilde{\lambda})$原对偶可行
    \[f_i(\tilde{x})\leq 0,i=1,\ldots,m,\tilde{\lambda}\geq 0\]
    因$p^\star=d^\star$，有
    \[\begin{aligned}
        f_0(\tilde{x})&=g(\tilde{\lambda})\\
        &=\inf_x\{f_0(x)+\sum_{i=1}^m\tilde{\lambda}_i f_i(x)\}\\
        \leq & f_0(\tilde{x})+\sum_{i=1}^m\tilde{\lambda}_i f_i(\tilde{x})\\
        \leq & f_0(\tilde{x})
    \end{aligned}\]
    进而不等号都得为等号
    \begin{enumerate}
        \item $\inf_x L(x,\tilde{\lambda})=L(\tilde{x},\tilde{\lambda})$
        \item $f_0(\tilde{x})=\sup_{\lambda\geq 0}\{f_0(\tilde{x})+\sum_{i=1}^m\lambda_i f_i(\tilde{x})\}=\sup_{\lambda\geq 0}L(\tilde{x},\lambda)$
    \end{enumerate}
    \[\implies L(\tilde{x},\tilde{\lambda})=\sup_{\lambda\geq 0}L(\tilde{x},\lambda)\]
    \[\implies (\tilde{x},\tilde{\lambda})\text{是}L(x,\lambda)\text{的鞍点}\]
\end{analysis}

一般优化问题的对偶理论
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{f_i(x)}{\leq 0}{,\quad i=1,\ldots,m}
    \addConstraint{h_i(x)}{=0}{,\quad i=1,\ldots,p}
\end{mini*}
不一定是凸问题，但$p^\star=d^\star$，最优解满足什么条件？

对偶问题
\begin{maxi*}
    {}{g(\lambda,v)}{}{}
    \addConstraint{\lambda}{\geq 0}
\end{maxi*}
\begin{analysis}
    设$(x^\star,\lambda^\star,v^\star)$为原对偶最优解，则$(x^\star,\lambda^\star,v^\star)$为原对偶可行解
    \[f_i(x^\star)\leq 0,i=1,\ldots,m,\quad h_i(x^\star)=0,i=1,\ldots,p,\quad\lambda^\star\geq 0\]
    \[\begin{aligned}
        p^\star=d^\star\implies f_0(x^\star)&=g(\lambda^\star,v^\star)\\
        &=\inf_x\{f_0(x)+\sum_{i=1}^m\lambda_i^\star f_i(x)+\sum_{i=1}^pv_i^\star h_i(x)\}\\
        &\leq f_0(x^\star)+\sum_{i=1}^m\lambda_i^\star f_i(x^\star)+\sum_{i=1}^pv_i^\star h_i(x^\star)\\
        &\leq f_0(x^\star)
    \end{aligned}\]
    同上理，不等号全取等
    \begin{enumerate}
        \item $\lambda_i^\star f_i(x^\star)=0,\forall i=1,\ldots,m$
        \item $x^\star=\arg\min_x L(x,\lambda^\star,v^\star)$
    \end{enumerate}
    若$f_0,f_i,h_i$均可微，则必要条件为
    \[\pd{L(x,\lambda^\star,v^\star)}{x}\Big|_{x=x^\star}=0\]
\end{analysis}
可微优化问题的KKT(Karush-Kuhn-Tucker)条件
\begin{itemize}
    \item $f_i(x^\star)\leq 0,i=1,\ldots,m$ primal feasibility
    \item $h_i(x^\star)=0,i=1,\ldots,p$ primal feasibility
    \item $\lambda^\star\geq 0$ dual feasibility
    \item $\lambda_i^\star f_i(x^\star)=0, i=1,\ldots,m$ complementarity slackness(对偶互斥条件)
    \item $\pd{L(x,\lambda^\star,v^\star)}{x}\Big|_{x=x^\star}=0$ stablity
\end{itemize}

\begin{theorem}
若原问题为凸，则KKT条件为\textbf{充要条件}
\end{theorem}
\begin{analysis}
    必要性已证，证明充分性\\
    若$(\tilde{x},\tilde{\lambda},\tilde{v})$满足KKT条件$\implies(\tilde{x},\tilde{\lambda},\tilde{v})$最优
    $\tilde{x}$为原问题可行解，$(\tilde{\lambda},\tilde{v})$为对偶问题可行解\\
    证明思路：$g(\tilde{\lambda},\tilde{v})=f_0(\tilde{x})$\\
    $L(x,\tilde{\lambda},\tilde{v})$为$x$的凸函数，则$\tilde{x}$使$L(x,\tilde{\lambda},\tilde{v})$最小
    \[\begin{aligned}
        g(\tilde{\lambda},\tilde{v})&=\inf_x L(x,\tilde{\lambda},\tilde{v})\\
        &=L(\tilde{x},\tilde{\lambda},\tilde{v})\\
        &=f_0(\tilde{x})+\sum_{i=1}^m\tilde{\lambda}_if_i(\tilde{x})+\sum_{i=1}^p\tilde{v_i}h_i(\tilde{x})\\
        &=f_0(\tilde{x})
    \end{aligned}\]
\end{analysis}

\begin{example}[Waterfilling算法]
    共$n$个信道(channel)
    \begin{center}
        \begin{tikzcd}
            \text{source}\arrow{r} & \text{destination}\arrow{l}
        \end{tikzcd}
    \end{center}
    \begin{mini*}
        {}{-\sum_{i=1}^n\log(\alpha_i+x_i)}{}{}
        \addConstraint{x}{\geq 0}
        \addConstraint{\vone^\T}{=1}
    \end{mini*}
\end{example}
\begin{analysis}
    KKT条件
    \begin{itemize}
        \item $x^\star\geq 0$
        \item $\vone^\T x^\star=1$
        \item $\lambda^\star\geq 0$
        \item $x_i^\star\lambda_i^\star=0,\forall i$
    \end{itemize}
\[\begin{aligned}
    L(x,\lambda,v)=-\sum_{i=1}^n\log(\alpha_i+x_i)-\lambda^\T x+v(\vone^\T x-1)
\end{aligned}\]
\[\lrp{\pd{L(x,\lambda,v)}{x}}_i=-\frac{1}{\alpha_i+x_i}-\lambda_i+v\]
\[-\frac{1}{\alpha_i+x_i^\star}-\lambda_i^\star+v^\star=0,\forall i\]
\[\implies v^\star\frac{1}{\alpha_i+x_i^\star},i=1,\ldots,n\]
\[x_i^\star\lrp{v^\star-\frac{1}{\alpha_i+x_i^\star}}=0,i=1,\ldots,n\]
若$v^\star>\frac{1}{\alpha_i}\implies x_i^\star=0$\\
若$v^\star<\frac{1}{\alpha_i}$
\[\frac{1}{\alpha_i}>v^\star\geq\frac{1}{\alpha_i+x_i^\star}\]
进而
\[\begin{aligned}
    x_i^\star&>0\\
    v^\star&=\frac{1}{\alpha_i+x_i^\star}\\
    x_i^\star&=\frac{1}{v^\star}-\alpha_i
\end{aligned}\]
\[\implies x_i^\star=\max\{0,\frac{1}{v^\star}-\alpha_i\}\]
结合$\sum_i x_i^\star=1$，即注水算法
\end{analysis}

Motivation：误差，调整参数测灵敏度
\begin{mini*}
    {}{f_0(x)}{}{}
    \addConstraint{f(x)}{\leq u_i}{,\quad i=1,\ldots,m}
    \addConstraint{h_i(x)}{= w_i}{,\quad i=1,\ldots,p}
\end{mini*}
新问题的最优解记为$p^\star(\vu,\vw)$

性质：若原始问题为凸，则$p^\star(\vu,\vw)$是$(u,w)$的凸函数

布尔线性规划问题做松弛(relaxation)
\[x_i\in\{0,1\}\implies 1\geq x_i\geq 0\]